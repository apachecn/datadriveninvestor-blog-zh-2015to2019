# Ai POS-(è¯æ€§)tagger #PART-1

> åŸæ–‡ï¼š<https://medium.datadriveninvestor.com/ai-pos-parts-of-speech-tagger-part-1-a3d6bd77ce5e?source=collection_archive---------5----------------------->

![](img/7645013ecd7da6ceef1f0c4bc3630633.png)

åœ¨å½“ä»Šä¸æ–­å‘å±•çš„äººå·¥æ™ºèƒ½é¢†åŸŸä¸­ï¼Œ[äººå·¥ç¥ç»ç½‘ç»œ](https://en.wikipedia.org/wiki/Artificial_neural_network)å·²ç»æˆåŠŸåœ°åº”ç”¨äºè®¡ç®—è¯æ€§æ ‡æ³¨ï¼Œå¹¶å–å¾—äº†å¾ˆå¥½çš„æ€§èƒ½ã€‚**è¯æ€§æ ‡æ³¨**å‡ ä¹æ˜¯ä»»ä½•è‡ªç„¶è¯­è¨€åˆ†æçš„ä¸»è¦ç»„æˆéƒ¨åˆ†ä¹‹ä¸€ã€‚å®ƒä»…ä»…æ„å‘³ç€ç”¨é€‚å½“çš„è¯ç±»å¦‚åè¯ã€åŠ¨è¯ç­‰æ¥æ ‡è®°å•è¯ã€‚

æˆ‘ä»¬è¿˜éœ€è¦ä¸ºæˆ‘ä»¬çš„æœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ æ¨¡å‹è®¾ç½®æ ‡ç­¾ã€‚NLTK é»˜è®¤ taggerï¼Œæ–¯å¦ç¦ CoreNLP taggerï¼Œå®¾å·æ ‘åº“ç­‰ã€‚æœ€è‘—åçš„æ˜¯[å®¾å·æ ‘æœ¨é“¶è¡Œã€‚](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html)æˆ‘ä»¬å°†åˆ©ç”¨è¿™ä¸€ç‚¹ã€‚

è¿™é‡Œæœ‰ä¸€ä¸ªç®€å•çš„è¯æ€§æ ‡æ³¨çš„ä¾‹å­ã€‚

```
from nltk import word_tokenize, pos_tag print pos_tag(word_tokenize("I'm learning NLP")) # [('I', 'PRP'), ("'m", 'VBP'), ('learning', 'VBG'), ('NLP', 'NNP')]
```

è™½ç„¶æœ‰å„ç§æ–¹æ³•å¯ä»¥ç”¨ Ai è¿›è¡Œè¯æ€§æ ‡æ³¨ï¼Œä½†æˆ‘ä»¬å°†æŠŠè¿™ä¸ªç³»åˆ—åˆ†æˆä¸‰éƒ¨åˆ†ï¼Œç¬¬ä¸€éƒ¨åˆ†(*ä½¿ç”¨å†³ç­–æ ‘*)ï¼Œ[ç¬¬äºŒéƒ¨åˆ†(*ä½¿ç”¨æ¡ä»¶éšæœºåœº* )](https://medium.com/@i_am_manish/ai-pos-parts-of-speech-tagger-part-2-fcf55f891d10) ï¼Œ[ç¬¬ä¸‰éƒ¨åˆ†](https://medium.com/@i_am_manish/ai-pos-parts-of-speech-tagger-part-3-c94b116a93e3) ( *ä½¿ç”¨ LSTMs/GRUs)ã€‚é‚£ä¹ˆï¼Œè®©æˆ‘ä»¬å¼€å§‹ç¬¬ä¸€éƒ¨åˆ†å§ã€‚*

ç¬¬ 1 éƒ¨åˆ†-ä½¿ç”¨å†³ç­–æ ‘

é¦–å…ˆï¼Œæˆ‘ä»¬ä¸‹è½½å¸¦æ³¨é‡Šçš„è¯­æ–™åº“:

```
import nltk nltk.download('treebank')
```

æ­£åœ¨åŠ è½½å¸¦æ ‡ç­¾çš„å¥å­â€¦

```
from nltk.corpus import treebank sentences = treebank.tagged_sents(tagset='universal')import random print(random.choice(sentences))
```

è¿™äº§ç”Ÿäº†ï¼Œ

(æœ¯è¯­ï¼Œæ ‡ç­¾)ä½œä¸º

```
[('Mr.', 'NOUN'), ('Otero', 'NOUN'), (',', '.'), ('who', 'PRON'), ('apparently', 'ADV'), ('has', 'VERB'), ('an', 'DET'), ('unpublished', 'ADJ'), ('number', 'NOUN'), (',', '.'), ('also', 'ADV'), ('could', 'VERB'), ("n't", 'ADV'), ('be', 'VERB'), ('reached', 'VERB'), ('.', '.')]print "Tagged sentences: ", len(tagged_sentences) print "Tagged words:", len(nltk.corpus.treebank.tagged_words()) ** # Tagged sentences:  3914** **# Tagged words: 100676**
```

*è¿™å˜æˆäº†ä¸€ä¸ªå¤šç±»åˆ†ç±»é—®é¢˜ï¼Œæœ‰å››åå¤šä¸ªä¸åŒçš„ç±»ã€‚äºæ˜¯ï¼Œ* [*ç›‘ç£äº†*](https://dataconomy.com/2015/01/whats-the-difference-between-supervised-and-unsupervised-learning/) *çš„é—®é¢˜ã€‚*

æ¥ä¸‹æ¥ï¼Œç‰¹å¾å·¥ç¨‹â€¦

æˆ‘ä»¬çš„åŠŸèƒ½éå¸¸ç®€å•ã€‚å¯¹äºæ¯ä¸ªæœ¯è¯­ï¼Œæˆ‘ä»¬æ ¹æ®ä»ä¸­æå–æœ¯è¯­çš„å¥å­åˆ›å»ºä¸€ä¸ªç‰¹å¾å­—å…¸ã€‚è¿™äº›å±æ€§å¯ä»¥åŒ…æ‹¬å…³äºå‰ä¸€ä¸ªå’Œåä¸€ä¸ªå•è¯ä»¥åŠå‰ç¼€å’Œåç¼€çš„ä¿¡æ¯ã€‚ä½†æ˜¯æˆ‘ä»¬å¯ä»¥è¿™æ ·æƒ³ï¼Œ**ä¸¤ä¸ªå­—æ¯çš„åç¼€æ˜¯è¿‡å»æ—¶æ€åŠ¨è¯**çš„ä¸€ä¸ªå¾ˆå¥½çš„æŒ‡ç¤ºå™¨ï¼Œä»¥*â€˜edâ€™*ç»“å°¾ï¼Œä¸‰ä¸ªå­—æ¯çš„åç¼€æœ‰åŠ©äºè¯†åˆ«ä»¥ *ing* ç»“å°¾çš„ç°åœ¨åˆ†è¯ã€‚

```
def features(sentence, index):
 //sentence: [w1, w2, ...],
index: the index of the word     
return {  'word': sentence[index], 
'is_first': index == 0, 
'is_last': index == len(sentence) - 1,    
'is_capitalized': sentence[index][0].upper() == sentence[index][0],      'is_all_caps': sentence[index].upper() == sentence[index],     'is_all_lower': sentence[index].lower() == sentence[index],        'prefix-1': sentence[index][0],       
'prefix-2': sentence[index][:2],      
'prefix-3': sentence[index][:3],       
'suffix-1': sentence[index][-1],     
'suffix-2': sentence[index][-2:],      
'suffix-3': sentence[index][-3:],     
'prev_word': '' if index == 0 else sentence[index - 1],    'next_word': '' if index == len(sentence) - 1 else sentence[index + 1],   
'has_hyphen': '-' in sentence[index],  
'is_numeric': sentence[index].isdigit(),   
'capitals_inside': sentence[index][1:].lower() != sentence[index][1:]  }  
import pprint  pprint.pprint(features(['This', 'is', 'a', 'sentence'], 2)) {'capitals_inside': False, 'has_hyphen': False, 'is_all_caps': False, 'is_all_lower': True, 'is_capitalized': False, 'is_first': False, 'is_last': False, 'is_numeric': False, 'next_word': 'sentence', 'prefix-1': 'a', 'prefix-2': 'a', 'prefix-3': 'a', 'prev_word': 'is', 'suffix-1': 'a', 'suffix-2': 'a', 'suffix-3': 'a', 'word': 'a'}
```

æˆ‘ä»¬ä¸ºæ¯ä¸ªå¸¦æ ‡ç­¾çš„æœ¯è¯­ç§»é™¤æ ‡ç­¾

```
def untag(tagged_sentence):   return [w for w, t in tagged_sentence]
```

ç°åœ¨ï¼Œå°±åƒåœ¨æœºå™¨å­¦ä¹ ä¸­ä¸€æ ·ï¼Œæˆ‘ä»¬éœ€è¦åˆ†å‰²æ•°æ®é›†æ¥è¿›è¡Œè®­ç»ƒå’Œæµ‹è¯•ã€‚æˆ‘ä»¬é¢„ç•™äº†. 75*len(tagged_sentences)ä¹‹å‰çš„æ ‡è®°å¥è¿›è¡Œè®­ç»ƒå’Œä¼‘æ¯æµ‹è¯•ã€‚

```
part=int(.75 * len(tagged_sentences)) training_sentences = tagged_sentences[:part] test_sentences = tagged_sentences[part:] def transform_to_dataset(tagged_sentences):  X, y = [], []      for tagged in tagged_sentences:         for index in range(len(tagged)):    X.append(features(untag(tagged), index))              y.append(tagged[index][1])       return X, y  X, y = transform_to_dataset(training_sentences)
```

ç°åœ¨ï¼Œæˆ‘ä»¬çš„ç¥ç»ç½‘ç»œå°†å‘é‡ä½œä¸ºè¾“å…¥ï¼Œå› æ­¤æˆ‘ä»¬éœ€è¦å°†æˆ‘ä»¬çš„å­—å…¸ç‰¹å¾è½¬æ¢ä¸ºå‘é‡ã€‚
ç»§ç»­ï¼Œ [sklearn](http://scikit-learn.org/) æœ‰ä¸€ä¸ªåä¸º[å­—å…¸çŸ¢é‡å™¨](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html)çš„å†…ç½®å‡½æ•°ï¼Œå®ƒæä¾›äº†ä¸€ä¸ªç®€å•çš„æ–¹æ³•æ¥å®Œæˆè¿™é¡¹å·¥ä½œã€‚å®ƒçš„å®ç°æ˜¯

```
from sklearn.feature_extraction import DictVectorizer # Fit our DictVectorizer with our set of features dict_vectorizer = DictVectorizer(sparse=False) dict_vectorizer.fit(X,y)
```

åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬éƒ½è®¾ç½®ä¸ºè®­ç»ƒåˆ†ç±»å™¨ï¼Œè¿™é‡Œæ˜¯[å†³ç­–æ ‘åˆ†ç±»å™¨](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)ã€‚æˆ‘ä»¬å¯ä»¥åœ¨è¿™é‡Œç”¨ n ä¸ªåˆ†ç±»å™¨è¿›è¡Œå®éªŒã€‚

```
from sklearn.tree import DecisionTreeClassifier from sklearn.feature_extraction import DictVectorizer from sklearn.pipeline import Pipeline ## w**e will create a pipeline including DictVectorizer & our classifier.It's easier that way round.** classifier=Pipeline([('vectorizer', DictVectorizer(sparse=False)),('classifier', DecisionTreeClassifier(criterion='entropy'))])  classifier.fit(X[:10000], y[:10000])   # Use only the first 10K samples if you're running it multiple times. It takes a fair bit ğŸ™‚Here, we are done with the training part. Next,  X_test, y_test = transform_to_dataset(test_sentences)
```

ç°åœ¨ï¼Œæˆ‘ä»¬æµ‹è¯•æˆ‘ä»¬çš„æ¨¡å‹çš„å‡†ç¡®æ€§â€¦

```
print "Accuracy:", clf.score(X_test, y_test)
```

å¯¹äºå†³ç­–æ ‘åˆ†ç±»å™¨æ¥è¯´ï¼Œåœ¨è¿™æ ·çš„æ•°æ®ä¸Šï¼Œè¿™ä¸æ˜¯å¤ªå·®çš„å‡†ç¡®æ€§ã€‚

è®©æˆ‘ä»¬æµ‹è¯•æˆ‘ä»¬çš„åˆ†ç±»å™¨ï¼Œ

```
def pos_tag(sentence): tags = clf.predict([features(sentence, index) for index in range(len(sentence))]) return zip(sentence, tags) print pos_tag(word_tokenize('This is my friend, Manish.'))
```

[('This 'ï¼Œu'DT ')ï¼Œ(' is 'ï¼Œu'VBZ ')ï¼Œ(' my 'ï¼Œu'JJ ')ï¼Œ(' friend 'ï¼Œu'NN ')ï¼Œ('ï¼Œ'ï¼Œu 'ï¼Œ')ï¼Œ(' Manish 'ï¼Œu'NNP ')ï¼Œ('ã€‚ï¼Œu 'ã€‚')]

æˆ‘ä»¬å¯ä»¥æé«˜ç²¾ç¡®åº¦ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨ä»¥ä¸‹å·¥å…·è¿›è¡Œè°ƒæ•´:

*   pystruct
*   å„ç§åˆ†ç±»å™¨
*   ä¸‰å…ƒæ ‡è®°ç¬¦
*   ä½¿ç”¨ä¸åŒçš„æ•°æ®é›†å’Œæ›´å¤§çš„æ•°æ®é›†

å¦‚æœä½ æƒ³çŸ¥é“å¦‚ä½•ä½¿ç”¨æ¡ä»¶éšæœºå­—æ®µ crf æˆ– keras(LSTMs/GRUs)è¿›è¡Œè¯æ€§æ ‡æ³¨ï¼Œè¯·é˜…è¯»[ç¬¬äºŒéƒ¨åˆ†](https://medium.com/@i_am_manish/ai-pos-parts-of-speech-tagger-part-2-fcf55f891d10) & [ç¬¬ä¸‰éƒ¨åˆ†](https://medium.com/@i_am_manish/ai-pos-parts-of-speech-tagger-part-3-c94b116a93e3)

ä¹Ÿçœ‹çœ‹æˆ‘æ”¶é›†çš„ä¸€äº›æœ‰ç”¨çš„èµ„æº:

[https://nlp.stanford.edu/software/tagger.shtml](https://nlp.stanford.edu/software/tagger.shtml)

[https://medium . freecodecamp . org/an-introduction-to-part-of-speech-tagging-and-the-hidden-Markov-model-953d 45338 f24](https://medium.freecodecamp.org/an-introduction-to-part-of-speech-tagging-and-the-hidden-markov-model-953d45338f24)

[https://www.nltk.org/book/ch05.html](https://www.nltk.org/book/ch05.html)

[https://becoming human . ai/è¯æ€§æ ‡æ³¨æ•™ç¨‹æ·±åº¦å­¦ä¹ åº“-d7f93fa05537](https://becominghuman.ai/part-of-speech-tagging-tutorial-with-the-keras-deep-learning-library-d7f93fa05537)

*åŸè½½äº 2018 å¹´ 11 æœˆ 3 æ—¥ blog.lipishala.com**çš„*[T22ã€‚](https://blog.lipishala.com/2018/11/03/ai-pos-parts_of_speech-tagger/)